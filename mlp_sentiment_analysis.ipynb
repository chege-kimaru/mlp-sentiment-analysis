{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp-sentiment-analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7OlLTF1YxIUk3alTZGJBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chege-kimaru/mlp-sentiment-analysis/blob/main/mlp_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JpxHFSGh2zt"
      },
      "source": [
        "**Explore the IMDB dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br7GDReih6hu",
        "outputId": "377fdf5d-0a98-4bfe-eae2-a5c219986201"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# download the tsv file from the github repo\n",
        "# upload it in your drive, copy the path and set it below\n",
        "labeledTrainDataPath = '/content/drive/My Drive/colab-data/labeledTrainData.tsv'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecEN6-_piVvv",
        "outputId": "91ae0091-03a9-43b4-8572-38e835f7ea80"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        " \n",
        "def clean_review(text):\n",
        "    # Strip HTML tags\n",
        "    text = re.sub('<[^<]+?>', ' ', text)\n",
        " \n",
        "    # Strip escaped quotes\n",
        "    text = text.replace('\\\\\"', '')\n",
        " \n",
        "    # Strip quotes\n",
        "    text = text.replace('\"', '')\n",
        " \n",
        "    return text\n",
        " \n",
        "df = pd.read_csv(labeledTrainDataPath, sep='\\t', quoting=3)\n",
        " \n",
        "# Create a cleaned_review column\n",
        "df['cleaned_review'] = df['review'].apply(clean_review)\n",
        " \n",
        "# Check out how the cleaned review compares to the original one\n",
        "print(df['review'][0])\n",
        "print(\"\\n\\n\")\n",
        "print(df['cleaned_review'][0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
            "\n",
            "\n",
            "\n",
            "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.  Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.  The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.  Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.  Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFWTK4cWij1F"
      },
      "source": [
        "**Bag of Words (BOW)**\n",
        "Basically, with BOW, we need to compute the vocabulary (all possible words) and then a text is represented by a vector having 1 (or the number of appearances) for the present words in the text and 0 for all the other indices.\n",
        "Beow is a very simplified and not optimized BOW transformer. But that is basically the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZW3cy3qilR7",
        "outputId": "4e3e42f1-0b17-4733-f675-2eb8bcba4c4c"
      },
      "source": [
        "VOCABULARY = ['dog', 'cheese', 'cat', 'mouse']\n",
        "TEXT = 'the mouse ate the cheese'\n",
        " \n",
        "def to_bow(text):\n",
        "    words = text.split(\" \")\n",
        "    return [1 if w in words else 0 for w in VOCABULARY]\n",
        " \n",
        "print(to_bow(TEXT))  # [0, 1, 0, 1]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1oHqdDSixaQ"
      },
      "source": [
        "We can now use vectorizers of Scikit Learn to transform a text to its BOW representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-kE5DOYiyWl",
        "outputId": "0be2bec1-9af3-4b66-bafb-f8f5e0e09b9b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        " \n",
        " \n",
        "vectorizer = CountVectorizer(vocabulary=VOCABULARY)\n",
        "vectorizer.transform([TEXT]).todense()  # matrix([[0, 1, 0, 1]])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4puMPpTMi9we"
      },
      "source": [
        "**Logistic Regression**\n",
        "- For each feature (numeric feature) the LogisticRegression has an associated weight\n",
        "- When classifying a feature vector, we multiply the features with their weights (w * x). We apply a non-linear function to this value that maps the result in the **[0, 1]** space.\n",
        "- If the resulting value is in the [0, 0.5) range than the data point belongs to the first class. If it belongs to the (0.5, 1] range then the data point belongs to the second class.\n",
        "- The tricky part is figuring out the weights of the model. We do this using the **Gradient Descent method**. This is called training the model. We optimize the weights so that the number of misclassified points is minimum.\n",
        "- GradientDescent is an iterative algorithm. It’s not a formula we need to apply. It is an exploratory process that tries to find the best-suited values. There are other training methods, but Gradient Descent and variants of it is what it’s used in practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaOgzMpmi6ww",
        "outputId": "467a2af1-5374-44aa-ee8b-ea8c0fcf4947"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Shuffle the data and then split it, keeping 20% aside for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], test_size=0.2)\n",
        " \n",
        "vectorizer = CountVectorizer(lowercase=True)\n",
        "vectorizer.fit(X_train)\n",
        " \n",
        "# classifier = Perceptron()\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(vectorizer.transform(X_train), y_train)\n",
        " \n",
        "print(\"Score:\", classifier.score(vectorizer.transform(X_test), y_test))  # Score: 0.8778"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.8838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDf_ebHHjFVY"
      },
      "source": [
        "**Going from training a LogisticRegression model to training a NeuralNetwork is easy peasy with Scikit-Learn**\n",
        "- Notice the changes made: we used the MLPClassifier instead of LogisticRegression\n",
        "- Let’s talk about the hidden_layer_sizes parameter. A neural network consists of layers. Every neural network has an input layer (size equal to the number of features) and an output layer (size equal to the number of classes). Between these two layers, there can be a number of hidden layers. The sizes of the hidden layers are a parameter. In this case we’ve only used a single hidden layer. Layers are composed of hidden units (or neurons). Each hidden unit is basically a LogisticRegression unit (with some notable differences, but close enough). This means that there are 100 LogisticRegression units doing their own thing.\n",
        "- The training of a neural network is done via BackPropagation which is a form of propagating the errors from the output layer all the way to the input layer and adjusting the weights incrementally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLuXoW_XjLlE",
        "outputId": "fbc71efc-dfb0-4da2-fca6-cfe433f23892"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Shuffle the data and then split it, keeping 20% aside for testing\n",
        "# x - features y - targets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], test_size=0.2)\n",
        " \n",
        "vectorizer = CountVectorizer(lowercase=True)\n",
        "vectorizer.fit(X_train)\n",
        " \n",
        "#  play around with the number of hidden layers to see the score\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(50,))\n",
        "classifier.fit(vectorizer.transform(X_train), y_train)\n",
        " \n",
        "print(\"Score:\", classifier.score(vectorizer.transform(X_test), y_test))  # Score: 0.8816"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.8786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mrk8JCLjZeM"
      },
      "source": [
        "**Test the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_iBJFnPjd4I",
        "outputId": "0d9ad980-eff3-48ed-82a7-e573e20add49"
      },
      "source": [
        "reviews = ['Best movie ever', 'I have watched better', 'This was an amazing movie']\n",
        "classifier.predict(vectorizer.transform(reviews))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}